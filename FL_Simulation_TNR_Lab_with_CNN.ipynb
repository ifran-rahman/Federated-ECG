{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ifran-rahman/Federated-ECG/blob/master/FL_Simulation_TNR_Lab_with_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "🔗 Mount Google Drive (if needed in Colab)"
      ],
      "metadata": {
        "id": "MCK8fwUAcrjB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRE6108ncpff",
        "outputId": "d14f94df-20fb-4eed-bfea-a1d9fb4c67f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "📦 Install Required Packages"
      ],
      "metadata": {
        "id": "9BBEcm6icupE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install h5py\n",
        "!pip install typing-extensions\n",
        "!pip install wheel\n",
        "!pip install -U flwr[\"simulation\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3GZeioIcthj",
        "outputId": "8d747caa-6ee1-406f-bdb1-329c71c4b669"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (3.13.0)\n",
            "Requirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.11/dist-packages (from h5py) (2.0.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (4.13.1)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (0.45.1)\n",
            "Requirement already satisfied: flwr[simulation] in /usr/local/lib/python3.11/dist-packages (1.17.0)\n",
            "Requirement already satisfied: cryptography<45.0.0,>=44.0.1 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (44.0.2)\n",
            "Requirement already satisfied: grpcio!=1.65.0,<2.0.0,>=1.62.3 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (1.71.0)\n",
            "Requirement already satisfied: iterators<0.0.3,>=0.0.2 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (0.0.2)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (2.0.2)\n",
            "Requirement already satisfied: pathspec<0.13.0,>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (0.12.1)\n",
            "Requirement already satisfied: protobuf<5.0.0,>=4.21.6 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (4.25.6)\n",
            "Requirement already satisfied: pycryptodome<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (3.22.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=6.0.2 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (6.0.2)\n",
            "Requirement already satisfied: ray==2.31.0 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (2.31.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (2.32.3)\n",
            "Requirement already satisfied: rich<14.0.0,>=13.5.0 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (13.9.4)\n",
            "Requirement already satisfied: tomli<3.0.0,>=2.0.1 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (2.2.1)\n",
            "Requirement already satisfied: tomli-w<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (1.2.0)\n",
            "Requirement already satisfied: typer<0.13.0,>=0.12.5 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (0.12.5)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from ray==2.31.0->flwr[simulation]) (8.1.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from ray==2.31.0->flwr[simulation]) (3.18.0)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.11/dist-packages (from ray==2.31.0->flwr[simulation]) (4.23.0)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ray==2.31.0->flwr[simulation]) (1.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from ray==2.31.0->flwr[simulation]) (24.2)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.11/dist-packages (from ray==2.31.0->flwr[simulation]) (1.3.2)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.11/dist-packages (from ray==2.31.0->flwr[simulation]) (1.5.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography<45.0.0,>=44.0.1->flwr[simulation]) (1.17.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->flwr[simulation]) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->flwr[simulation]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->flwr[simulation]) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->flwr[simulation]) (2025.1.31)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich<14.0.0,>=13.5.0->flwr[simulation]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich<14.0.0,>=13.5.0->flwr[simulation]) (2.18.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from typer<0.13.0,>=0.12.5->flwr[simulation]) (4.13.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<0.13.0,>=0.12.5->flwr[simulation]) (1.5.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography<45.0.0,>=44.0.1->flwr[simulation]) (2.22)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.5.0->flwr[simulation]) (0.1.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray==2.31.0->flwr[simulation]) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray==2.31.0->flwr[simulation]) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray==2.31.0->flwr[simulation]) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray==2.31.0->flwr[simulation]) (0.24.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "📚 Imports"
      ],
      "metadata": {
        "id": "EaZxL5cHcxau"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import tracemalloc\n",
        "import sys\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, TensorDataset, SubsetRandomSampler, random_split\n",
        "from torch import nn, optim\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import flwr as fl\n",
        "from collections import OrderedDict\n",
        "from typing import Dict, List, Tuple, Optional, Union\n",
        "from flwr.common import NDArrays, Scalar, Metrics"
      ],
      "metadata": {
        "id": "ZX9kFpNCcv6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "🧠 Memory and Device Setup"
      ],
      "metadata": {
        "id": "RZHF3uDGc1jj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tracemalloc.start()\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "CWbTq7kFc0E8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "🧪 Dataset Preparation Function"
      ],
      "metadata": {
        "id": "5G_SJthSc3ge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def prepare__dataset(abnormal: pd.DataFrame, normal: pd.DataFrame, val_split_factor: float) -> tuple[torch.utils.data.Dataset, torch.utils.data.Dataset, dict]:\n",
        "    \"\"\"Prepares the training and validation datasets for ECG classification, shuffling the data before splitting.\n",
        "\n",
        "    Args:\n",
        "        abnormal (pd.DataFrame): DataFrame containing abnormal ECG data.\n",
        "        normal (pd.DataFrame): DataFrame containing normal ECG data.\n",
        "        val_split_factor (float): Fraction of the data to use for validation.\n",
        "\n",
        "    Returns:\n",
        "        tuple[torch.utils.data.Dataset, torch.utils.data.Dataset, dict]: A tuple containing the training dataset,\n",
        "            the validation dataset, and a dictionary with the number of examples in each.\n",
        "    \"\"\"\n",
        "    abnormal = abnormal.drop([187], axis=1)\n",
        "    normal = normal.drop([187], axis=1)\n",
        "\n",
        "    y_abnormal = np.ones((abnormal.shape[0]))\n",
        "    y_abnormal = pd.DataFrame(y_abnormal)\n",
        "\n",
        "    y_normal = np.zeros((normal.shape[0]))\n",
        "    y_normal = pd.DataFrame(y_normal)\n",
        "\n",
        "    x = pd.concat([abnormal, normal], sort=True)\n",
        "    y = pd.concat([y_abnormal, y_normal], sort=True)\n",
        "\n",
        "    x = x.to_numpy()\n",
        "    y = y[0].to_numpy()\n",
        "\n",
        "    # Create a TensorDataset before shuffling\n",
        "    full_dataset = torch.utils.data.TensorDataset(torch.from_numpy(x).float(),\n",
        "                                                  torch.from_numpy(y).long())\n",
        "\n",
        "    # Calculate the lengths for training and validation sets\n",
        "    full_len = len(full_dataset)\n",
        "    val_len = int(full_len * val_split_factor)\n",
        "    train_len = full_len - val_len\n",
        "\n",
        "    # Shuffle the dataset using a random permutation of indices\n",
        "    indices = torch.randperm(full_len).tolist()\n",
        "    train_indices = indices[:train_len]\n",
        "    val_indices = indices[train_len:]\n",
        "\n",
        "    # Create SubsetRandomSamplers to get shuffled subsets\n",
        "    train_sampler = torch.utils.data.SubsetRandomSampler(train_indices)\n",
        "    val_sampler = torch.utils.data.SubsetRandomSampler(val_indices)\n",
        "\n",
        "    # Create DataLoaders using the samplers\n",
        "    train_dataset = torch.utils.data.DataLoader(full_dataset, batch_size=train_len, sampler=train_sampler)\n",
        "    val_dataset = torch.utils.data.DataLoader(full_dataset, batch_size=val_len, sampler=val_sampler)\n",
        "\n",
        "    num_examples = {'trainset': train_len,\n",
        "                    'testset': val_len}\n",
        "\n",
        "    # Extract the datasets from the DataLoaders (since DataLoader returns iterators)\n",
        "    train_dataset = list(train_dataset)[0]\n",
        "    val_dataset = list(val_dataset)[0]\n",
        "\n",
        "    return train_dataset, val_dataset, num_examples"
      ],
      "metadata": {
        "id": "Ac2yUTFpc2ns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "🧩 CNN Model for ECG Classification"
      ],
      "metadata": {
        "id": "ZGZ3FjsSc6LG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ecg_net(nn.Module):\n",
        "\n",
        "    def __init__(self, num_of_class):\n",
        "        super(ecg_net, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv1d(1, 16, kernel_size=3, stride=1, padding=1),\n",
        "            nn.MaxPool1d(2),\n",
        "\n",
        "            nn.Conv1d(16, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.MaxPool1d(2),\n",
        "\n",
        "            nn.Conv1d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.MaxPool1d(2),\n",
        "\n",
        "        )\n",
        "\n",
        "        self.linear = nn.Sequential(\n",
        "            nn.Linear(2944,500),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "            nn.Linear(500, num_of_class),\n",
        "\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = x.unsqueeze(1)\n",
        "        x = self.model(x)\n",
        "        # print(x.shape)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        #x [b, 2944]\n",
        "        # print(x.shape)\n",
        "        x = self.linear(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "_AU_ux8ic4py"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "📈 Evaluation Function"
      ],
      "metadata": {
        "id": "XFwpv2Evc-jw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evalute(model, loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = len(loader)\n",
        "    val_bar = tqdm(loader, file=sys.stdout)\n",
        "    for x, y in val_bar:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        with torch.no_grad():\n",
        "            logits = model(x)\n",
        "            pred = logits.argmax(dim=1)\n",
        "        correct += torch.eq(pred, y).sum().float().item()\n",
        "    return correct / total"
      ],
      "metadata": {
        "id": "Ij1iwDwPc7wD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "🏋️‍♂️ Local Training Function"
      ],
      "metadata": {
        "id": "2L7UX9tVdBFd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_client(model, train_loader, valid_loader, epochs=1):\n",
        "    optimizer = optim.Adam(model.parameters(), lr=3e-3)\n",
        "    criteon = nn.CrossEntropyLoss()\n",
        "    best_acc, best_epoch, global_step = 0, 0, 0\n",
        "    for epoch in range(epochs):\n",
        "        train_bar = tqdm(train_loader, file=sys.stdout)\n",
        "        for step, (x, y) in enumerate(train_bar):\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            model.train()\n",
        "            logits = model(x)\n",
        "            loss = criteon(logits, y)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_bar.desc = f\"train epoch[{epoch+1}/{epochs}] loss:{loss:.3f}\"\n",
        "            global_step += 1\n",
        "        val_acc = evalute(model, valid_loader)\n",
        "        if val_acc > best_acc:\n",
        "            best_epoch = epoch\n",
        "            best_acc = val_acc\n",
        "    print('best acc:', best_acc, 'best epoch:', best_epoch)"
      ],
      "metadata": {
        "id": "-Vvj4ElOc_jU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"## One Client, One Data Partition\n",
        "\n",
        "To start designing a Federated Learning pipeline we need to meet one of the key properties in FL: each client has its own data partition. To accomplish this with the dataset, we are going to generate N random partitions, where N is the total number of clients in our FL system.\n",
        "\"\"\"\n",
        "\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "\n",
        "def get_dataset(abnormal, normal, val_split_factor): # done\n",
        "\n",
        "    abnormal = abnormal.drop([187], axis=1)\n",
        "    normal = normal.drop([187], axis=1)\n",
        "\n",
        "    y_abnormal = np.ones((abnormal.shape[0]))\n",
        "    y_abnormal = pd.DataFrame(y_abnormal)\n",
        "\n",
        "    y_normal = np.zeros((normal.shape[0]))\n",
        "    y_normal = pd.DataFrame(y_normal)\n",
        "\n",
        "    x = pd.concat([abnormal, normal], sort=True)\n",
        "    y = pd.concat([y_abnormal, y_normal] ,sort=True)\n",
        "\n",
        "    x = x.to_numpy()\n",
        "    y = y[0].to_numpy()\n",
        "\n",
        "    train_dataset = torch.utils.data.TensorDataset(torch.from_numpy(x).float(),\n",
        "                                                torch.from_numpy(y).long())\n",
        "\n",
        "    train_len = x.shape[0]\n",
        "    val_len = int(train_len * val_split_factor)\n",
        "    train_len -= val_len\n",
        "\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_len, val_len])\n",
        "\n",
        "    num_examples =  {'trainset': train_len,\n",
        "                    'testset': val_len}\n",
        "\n",
        "    return train_dataset, val_dataset, num_examples"
      ],
      "metadata": {
        "id": "AbjZxdTFma4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "🧠 Flower Client"
      ],
      "metadata": {
        "id": "_qOXuZisdEGe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FlowerClient(fl.client.NumPyClient):\n",
        "    def __init__(self, trainloader, vallodaer) -> None:\n",
        "        super().__init__()\n",
        "        self.trainloader = trainloader       # Local training DataLoader\n",
        "        self.valloader = vallodaer           # Local validation DataLoader\n",
        "        self.model = ecg_net(2).to(device)   # Initialize model and move to device (CPU/GPU)\n",
        "\n",
        "    def set_parameters(self, parameters):\n",
        "        # Convert received NumPy arrays into model state_dict format\n",
        "        params_dict = zip(self.model.state_dict().keys(), parameters)\n",
        "        state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n",
        "        self.model.load_state_dict(state_dict, strict=True)  # Load weights into the model\n",
        "\n",
        "    def get_parameters(self, config: Dict[str, Scalar]):\n",
        "        # Convert model weights from tensors to NumPy arrays to send to the server\n",
        "        return [val.cpu().numpy() for _, val in self.model.state_dict().items()]\n",
        "\n",
        "    def fit(self, parameters, config):\n",
        "        # Receive updated global parameters and train locally\n",
        "        self.set_parameters(parameters)\n",
        "        train_client(self.model, self.trainloader, self.valloader, epochs=5)\n",
        "        # Return updated local weights and the number of training examples\n",
        "        return self.get_parameters({}), len(self.trainloader), {}\n",
        "\n",
        "    def evaluate(self, parameters: NDArrays, config: Dict[str, Scalar]) -> Tuple[float, int, Dict[str, Scalar]]:\n",
        "        # Load latest global parameters\n",
        "        self.set_parameters(parameters)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        total_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        self.model.eval()\n",
        "        # Run evaluation on local validation set\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in self.valloader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = self.model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                total_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "        avg_loss = total_loss / len(self.valloader)  # Average loss per batch\n",
        "        accuracy = correct / total                   # Total accuracy\n",
        "        return avg_loss, total, {\"accuracy\": accuracy}\n"
      ],
      "metadata": {
        "id": "-wJavbBKdDCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "📊 Server Evaluation Function"
      ],
      "metadata": {
        "id": "xJ5OI91xdGsh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the global model\n",
        "model = ecg_net(2).to(device)\n",
        "\n",
        "# Define the server-side evaluation function\n",
        "def evaluate(server_round, parameters, config):\n",
        "    # Convert received parameters from server (NumPy arrays) to PyTorch state_dict\n",
        "    params_dict = zip(model.state_dict().keys(), parameters)\n",
        "    state_dict = OrderedDict({k: torch.tensor(v) for k, v in params_dict})\n",
        "    model.load_state_dict(state_dict, strict=True)  # Load weights into the model\n",
        "\n",
        "    # Loss function for evaluation\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Tracking variables for total loss and accuracy\n",
        "    total_loss = 0.0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    with torch.no_grad():  # Disable gradient calculation for efficiency\n",
        "        for valloader in valloaders:  # Iterate over all client validation loaders\n",
        "            for inputs, labels in valloader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = model(inputs)\n",
        "\n",
        "                # Compute loss\n",
        "                loss = criterion(outputs, labels)\n",
        "                total_loss += loss.item() * inputs.size(0)  # Scale by batch size\n",
        "\n",
        "                # Compute predictions and count correct ones\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                total_correct += (predicted == labels).sum().item()\n",
        "                total_samples += labels.size(0)\n",
        "\n",
        "    # Calculate average loss and accuracy over all validation data\n",
        "    avg_loss = total_loss / total_samples\n",
        "    accuracy = total_correct / total_samples\n",
        "\n",
        "    # Return the evaluation metrics as required by Flower\n",
        "    return avg_loss, {\"accuracy\": accuracy}"
      ],
      "metadata": {
        "id": "VTJ4Py4LdFDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "💾 Save Model Strategy"
      ],
      "metadata": {
        "id": "29gAuNA3dI65"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom FedAvg strategy that saves the aggregated global model after each round\n",
        "class SaveModelStrategy(fl.server.strategy.FedAvg):\n",
        "\n",
        "    def aggregate_fit(self, server_round, results, failures):\n",
        "        # Perform standard FedAvg aggregation from Flower\n",
        "        aggregated_parameters, aggregated_metrics = super().aggregate_fit(server_round, results, failures)\n",
        "\n",
        "        # If aggregation was successful\n",
        "        if aggregated_parameters is not None:\n",
        "            print(f\"Saving round {server_round} aggregated weights...\")\n",
        "\n",
        "            # Convert Flower Parameters object to list of NumPy arrays\n",
        "            aggregated_ndarrays = fl.common.parameters_to_ndarrays(aggregated_parameters)\n",
        "\n",
        "            # Save the aggregated weights as a .npz file (easily reloadable)\n",
        "            np.savez(f\"round-{server_round}-weights.npz\", *aggregated_ndarrays)\n",
        "\n",
        "            # Convert NumPy arrays to PyTorch state_dict format\n",
        "            params_dict = zip(model.state_dict().keys(), aggregated_ndarrays)\n",
        "            state_dict = OrderedDict({k: torch.tensor(v) for k, v in params_dict})\n",
        "\n",
        "            # Load aggregated weights into the global model\n",
        "            model.load_state_dict(state_dict, strict=True)\n",
        "\n",
        "            # Save the full PyTorch model checkpoint\n",
        "            torch.save(model.state_dict(), f\"model_round_{server_round}.pth\")\n",
        "\n",
        "        # Return the aggregated parameters and metrics back to Flower\n",
        "        return aggregated_parameters, aggregated_metrics"
      ],
      "metadata": {
        "id": "2pM74JF_dHv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "⚙️ FL Config and Client Generator"
      ],
      "metadata": {
        "id": "gSQ0MaKQdLZr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to customize training configuration for each round\n",
        "def fit_config(server_round):\n",
        "    # Return training configuration as a dictionary\n",
        "    # Use 1 local epoch for the first round, then increase to 2\n",
        "    return {\n",
        "        \"batch_size\": 1,\n",
        "        \"local_epochs\": 1 if server_round < 2 else 2\n",
        "    }\n",
        "\n",
        "# Function to compute a weighted average of accuracy across all clients\n",
        "def weighted_average(metrics: List[Tuple[int, Metrics]]) -> Metrics:\n",
        "    # Multiply each client's accuracy by its number of examples\n",
        "    accuracies = [num_examples * m[\"accuracy\"] for num_examples, m in metrics]\n",
        "    # Extract the number of examples per client\n",
        "    examples = [num_examples for num_examples, _ in metrics]\n",
        "    # Return the global (weighted) accuracy\n",
        "    return {\n",
        "        \"accuracy\": sum(accuracies) / sum(examples)\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "# Factory function to generate Flower clients based on client ID\n",
        "def generate_client_fn(trainloaders, valloaders):\n",
        "    # Inner function that Flower will call to instantiate clients\n",
        "    def client_fn(cid: str):\n",
        "        # Use the client ID to select corresponding train/val loaders\n",
        "        return FlowerClient(\n",
        "            trainloader=trainloaders[int(cid)],\n",
        "            vallodaer=valloaders[int(cid)]\n",
        "        )\n",
        "    return client_fn"
      ],
      "metadata": {
        "id": "kVoanK0vdJ9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "🚀 Federated Training"
      ],
      "metadata": {
        "id": "8d3n191xdN1E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the number of participating clients\n",
        "NUM_CLIENTS = 2\n",
        "\n",
        "# Prepare client datasets (train/val) and a test set (if needed)\n",
        "# NOTE: `prepare_dataset` must be defined to return loaders for each client\n",
        "trainloaders, valloaders, testloader = prepare_dataset(num_partitions=NUM_CLIENTS, batch_size=20, val_ratio=0.1)\n",
        "\n",
        "# Generate a function that returns the appropriate FlowerClient instance based on client ID\n",
        "client_fn_callback = generate_client_fn(trainloaders, valloaders)\n",
        "\n",
        "# Define the federated averaging strategy and specify custom behavior\n",
        "strategy = SaveModelStrategy(\n",
        "    fraction_fit=1.0,                  # Use all clients for training in each round\n",
        "    min_fit_clients=2,                # Minimum number of clients required to train\n",
        "    min_available_clients=2,          # Minimum clients that must be available to start the round\n",
        "    evaluate_fn=evaluate,             # Custom server-side evaluation function\n",
        "    on_fit_config_fn=fit_config       # Send training config to each client per round\n",
        ")\n",
        "\n",
        "# Start timing the training\n",
        "start_training = time.time()\n",
        "\n",
        "# Launch the Flower federated simulation\n",
        "fl.simulation.start_simulation(\n",
        "    client_fn=client_fn_callback,                     # Client selection function\n",
        "    num_clients=NUM_CLIENTS,                          # Total number of simulated clients\n",
        "    config=fl.server.ServerConfig(num_rounds=3),      # Number of global training rounds\n",
        "    strategy=strategy,                                # Federated strategy with model saving\n",
        "    client_resources={\"num_cpus\": 1, \"num_gpus\": 1},  # Resource allocation per client\n",
        "    ray_init_args={\"log_to_driver\": False, \"num_cpus\": 1, \"num_gpus\": 1}  # Ray backend config\n",
        ")\n",
        "\n",
        "# End timing\n",
        "end_training = time.time()\n",
        "\n",
        "# Print total training duration\n",
        "print('Total Training Time:', end_training - start_training)\n",
        "\n",
        "# Report memory usage\n",
        "current, peak = tracemalloc.get_traced_memory()\n",
        "print(f'Current memory [MB]: {current/(1024*1024):.2f}, Peak memory [MB]: {peak/(1024*1024):.2f}')\n",
        "tracemalloc.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "lvcq4NF9dMva",
        "outputId": "6dec3a19-a525-44f0-9d37-eb0008b6eb1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "prepare_dataset() got an unexpected keyword argument 'num_partitions'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-e952cda053c8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Prepare client datasets (train/val) and a test set (if needed)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# NOTE: `prepare_dataset` must be defined to return loaders for each client\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrainloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_partitions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_CLIENTS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Generate a function that returns the appropriate FlowerClient instance based on client ID\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: prepare_dataset() got an unexpected keyword argument 'num_partitions'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "FL Simulation TNR Lab with CNN\n",
        "Cleaned and commented version of original Colab-generated script\n",
        "\"\"\"\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Install required packages\n",
        "!pip install h5py\n",
        "!pip install typing-extensions\n",
        "!pip install wheel\n",
        "!pip install -U flwr[\"simulation\"]\n",
        "\n",
        "# Imports\n",
        "import time\n",
        "import tracemalloc\n",
        "import sys\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, TensorDataset, SubsetRandomSampler, random_split\n",
        "from torch import nn, optim\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import flwr as fl\n",
        "from collections import OrderedDict\n",
        "from typing import Dict, List, Tuple, Optional, Union\n",
        "from flwr.common import NDArrays, Scalar, Metrics\n",
        "\n",
        "# Start memory tracking\n",
        "tracemalloc.start()\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Prepare ECG dataset for training and validation\n",
        "def prepare_dataset(num_partitions: int, batch_size: int, val_ratio: float = 0.1):\n",
        "    \"\"\"This function partitions the training set into N disjoint\n",
        "    subsets, each will become the local dataset of a client. This\n",
        "    function also subsequently partitions each training set partition\n",
        "    into train and validation. The test set is left intact and will\n",
        "    be used by the central server to asses the performance of the\n",
        "    global model.\"\"\"\n",
        "\n",
        "    # get the datatset\n",
        "    trainset, testset, _ = get_dataset(abnormal=abnormal, normal=normal, val_split_factor=val_split_factor)\n",
        "\n",
        "    print(len(trainset))\n",
        "    print(len(testset))\n",
        "    # split trainset into `num_partitions` trainsets\n",
        "    num_images = len(trainset) // num_partitions\n",
        "\n",
        "    partition_len = [num_images] * num_partitions\n",
        "\n",
        "    partition_len[len(partition_len)-1] = len(trainset)-sum(partition_len[:-1])\n",
        "    print(partition_len)\n",
        "    trainsets = random_split(\n",
        "        trainset, partition_len, torch.Generator().manual_seed(2023)\n",
        "    )\n",
        "\n",
        "    # create dataloaders with train+val support\n",
        "    trainloaders = []\n",
        "    valloaders = []\n",
        "    for trainset_ in trainsets:\n",
        "        num_total = len(trainset_)\n",
        "        num_val = int(val_ratio * num_total)\n",
        "        num_train = num_total - num_val\n",
        "\n",
        "        for_train, for_val = random_split(\n",
        "            trainset_, [num_train, num_val], torch.Generator().manual_seed(2023)\n",
        "        )\n",
        "        trainloaders.append(\n",
        "            DataLoader(for_train, batch_size=batch_size, shuffle=True, num_workers=0)\n",
        "        )\n",
        "        valloaders.append(\n",
        "            DataLoader(for_val, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "        )\n",
        "\n",
        "    # create dataloader for the test set\n",
        "    testloader = DataLoader(testset, batch_size=128)\n",
        "\n",
        "    datapoint_count = len(trainset[0][0])\n",
        "    print('Minimum DataPoint required for a signal :', datapoint_count)\n",
        "    return trainloaders, valloaders, testloader\n",
        "\n",
        "# Define CNN model for ECG classification\n",
        "class ecg_net(nn.Module):\n",
        "    def __init__(self, num_of_class):\n",
        "        super(ecg_net, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv1d(1, 16, kernel_size=3, stride=1, padding=1),\n",
        "            nn.MaxPool1d(2),\n",
        "            nn.Conv1d(16, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.MaxPool1d(2),\n",
        "            nn.Conv1d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.MaxPool1d(2),\n",
        "        )\n",
        "        self.linear = nn.Sequential(\n",
        "            nn.Linear(2944, 500),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "            nn.Linear(500, num_of_class),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)\n",
        "        x = self.model(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return self.linear(x)\n",
        "\n",
        "# Evaluate model performance on a loader\n",
        "def evalute(model, loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = len(loader)\n",
        "    val_bar = tqdm(loader, file=sys.stdout)\n",
        "    for x, y in val_bar:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        with torch.no_grad():\n",
        "            logits = model(x)\n",
        "            pred = logits.argmax(dim=1)\n",
        "        correct += torch.eq(pred, y).sum().float().item()\n",
        "    return correct / total\n",
        "\n",
        "# Train the model on client's local data\n",
        "def train_client(model, train_loader, valid_loader, epochs=1):\n",
        "    optimizer = optim.Adam(model.parameters(), lr=3e-3)\n",
        "    criteon = nn.CrossEntropyLoss()\n",
        "    best_acc, best_epoch, global_step = 0, 0, 0\n",
        "    for epoch in range(epochs):\n",
        "        train_bar = tqdm(train_loader, file=sys.stdout)\n",
        "        for step, (x, y) in enumerate(train_bar):\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            model.train()\n",
        "            logits = model(x)\n",
        "            loss = criteon(logits, y)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_bar.desc = f\"train epoch[{epoch+1}/{epochs}] loss:{loss:.3f}\"\n",
        "            global_step += 1\n",
        "        val_acc = evalute(model, valid_loader)\n",
        "        if val_acc > best_acc:\n",
        "            best_epoch = epoch\n",
        "            best_acc = val_acc\n",
        "    print('best acc:', best_acc, 'best epoch:', best_epoch)\n",
        "\n",
        "# Define Flower client\n",
        "class FlowerClient(fl.client.NumPyClient):\n",
        "    def __init__(self, trainloader, vallodaer) -> None:\n",
        "        super().__init__()\n",
        "        self.trainloader = trainloader\n",
        "        self.valloader = vallodaer\n",
        "        self.model = ecg_net(2).to(device)\n",
        "\n",
        "    def set_parameters(self, parameters):\n",
        "        params_dict = zip(self.model.state_dict().keys(), parameters)\n",
        "        state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n",
        "        self.model.load_state_dict(state_dict, strict=True)\n",
        "\n",
        "    def get_parameters(self, config: Dict[str, Scalar]):\n",
        "        return [val.cpu().numpy() for _, val in self.model.state_dict().items()]\n",
        "\n",
        "    def fit(self, parameters, config):\n",
        "        self.set_parameters(parameters)\n",
        "        train_client(self.model, self.trainloader, self.valloader, epochs=5)\n",
        "        return self.get_parameters({}), len(self.trainloader), {}\n",
        "\n",
        "    def evaluate(self, parameters: NDArrays, config: Dict[str, Scalar]) -> Tuple[float, int, Dict[str, Scalar]]:\n",
        "        self.set_parameters(parameters)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        total_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in self.valloader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = self.model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                total_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "        avg_loss = total_loss / len(self.valloader)\n",
        "        accuracy = correct / total\n",
        "        return avg_loss, total, {\"accuracy\": accuracy}\n",
        "\n",
        "# Server strategy and training setup\n",
        "model = ecg_net(2).to(device)\n",
        "\n",
        "# Evaluation function for server\n",
        "def evaluate(server_round, parameters, config):\n",
        "    params_dict = zip(model.state_dict().keys(), parameters)\n",
        "    state_dict = OrderedDict({k: torch.tensor(v) for k, v in params_dict})\n",
        "    model.load_state_dict(state_dict, strict=True)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    total_loss, total_correct, total_samples = 0.0, 0, 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for valloader in valloaders:\n",
        "            for inputs, labels in valloader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                total_loss += loss.item() * inputs.size(0)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                total_correct += (predicted == labels).sum().item()\n",
        "                total_samples += labels.size(0)\n",
        "    avg_loss = total_loss / total_samples\n",
        "    accuracy = total_correct / total_samples\n",
        "    return avg_loss, {\"accuracy\": accuracy}\n",
        "\n",
        "# Strategy with model saving\n",
        "class SaveModelStrategy(fl.server.strategy.FedAvg):\n",
        "    def aggregate_fit(self, server_round, results, failures):\n",
        "        aggregated_parameters, aggregated_metrics = super().aggregate_fit(server_round, results, failures)\n",
        "        if aggregated_parameters is not None:\n",
        "            print(f\"Saving round {server_round} aggregated weights...\")\n",
        "            aggregated_ndarrays = fl.common.parameters_to_ndarrays(aggregated_parameters)\n",
        "            np.savez(f\"round-{server_round}-weights.npz\", *aggregated_ndarrays)\n",
        "            params_dict = zip(model.state_dict().keys(), aggregated_ndarrays)\n",
        "            state_dict = OrderedDict({k: torch.tensor(v) for k, v in params_dict})\n",
        "            model.load_state_dict(state_dict, strict=True)\n",
        "            torch.save(model.state_dict(), f\"model_round_{server_round}.pth\")\n",
        "        return aggregated_parameters, aggregated_metrics\n",
        "\n",
        "# Define config and metrics aggregation\n",
        "def fit_config(server_round):\n",
        "    return {\"batch_size\": 1, \"local_epochs\": 1 if server_round < 2 else 2}\n",
        "\n",
        "def weighted_average(metrics: List[Tuple[int, Metrics]]) -> Metrics:\n",
        "    accuracies = [num_examples * m[\"accuracy\"] for num_examples, m in metrics]\n",
        "    examples = [num_examples for num_examples, _ in metrics]\n",
        "    return {\"accuracy\": sum(accuracies) / sum(examples)}\n",
        "\n",
        "# Client generator\n",
        "def generate_client_fn(trainloaders, valloaders):\n",
        "    def client_fn(cid: str):\n",
        "        return FlowerClient(trainloader=trainloaders[int(cid)], vallodaer=valloaders[int(cid)])\n",
        "    return client_fn\n",
        "\n",
        "# Dataset preparation and training launch\n",
        "NUM_CLIENTS = 2\n",
        "trainloaders, valloaders, testloader = prepare_dataset(num_partitions=NUM_CLIENTS, batch_size=20, val_ratio=0.1)\n",
        "client_fn_callback = generate_client_fn(trainloaders, valloaders)\n",
        "strategy = SaveModelStrategy(\n",
        "    fraction_fit=1.0,\n",
        "    min_fit_clients=2,\n",
        "    min_available_clients=2,\n",
        "    evaluate_fn=evaluate,\n",
        "    on_fit_config_fn=fit_config,\n",
        ")\n",
        "\n",
        "start_training = time.time()\n",
        "fl.simulation.start_simulation(\n",
        "    client_fn=client_fn_callback,\n",
        "    num_clients=NUM_CLIENTS,\n",
        "    config=fl.server.ServerConfig(num_rounds=3),\n",
        "    strategy=strategy,\n",
        "    client_resources={\"num_cpus\": 1, \"num_gpus\": 1},\n",
        "    ray_init_args={\"log_to_driver\": False, \"num_cpus\": 1, \"num_gpus\": 1}\n",
        ")\n",
        "end_training = time.time()\n",
        "\n",
        "print('Total Training Time:', end_training - start_training)\n",
        "current, peak = tracemalloc.get_traced_memory()\n",
        "print(f'Current memory [MB]: {current/(1024*1024):.2f}, Peak memory [MB]: {peak/(1024*1024):.2f}')\n",
        "tracemalloc.stop()"
      ],
      "metadata": {
        "id": "VgYXysUadPn-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c0c11e56-ce20-47de-e591-d835bd83fe69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (3.13.0)\n",
            "Requirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.11/dist-packages (from h5py) (2.0.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (4.13.1)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (0.45.1)\n",
            "Requirement already satisfied: flwr[simulation] in /usr/local/lib/python3.11/dist-packages (1.17.0)\n",
            "Requirement already satisfied: cryptography<45.0.0,>=44.0.1 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (44.0.2)\n",
            "Requirement already satisfied: grpcio!=1.65.0,<2.0.0,>=1.62.3 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (1.71.0)\n",
            "Requirement already satisfied: iterators<0.0.3,>=0.0.2 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (0.0.2)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (2.0.2)\n",
            "Requirement already satisfied: pathspec<0.13.0,>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (0.12.1)\n",
            "Requirement already satisfied: protobuf<5.0.0,>=4.21.6 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (4.25.6)\n",
            "Requirement already satisfied: pycryptodome<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (3.22.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=6.0.2 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (6.0.2)\n",
            "Requirement already satisfied: ray==2.31.0 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (2.31.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (2.32.3)\n",
            "Requirement already satisfied: rich<14.0.0,>=13.5.0 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (13.9.4)\n",
            "Requirement already satisfied: tomli<3.0.0,>=2.0.1 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (2.2.1)\n",
            "Requirement already satisfied: tomli-w<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (1.2.0)\n",
            "Requirement already satisfied: typer<0.13.0,>=0.12.5 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (0.12.5)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from ray==2.31.0->flwr[simulation]) (8.1.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from ray==2.31.0->flwr[simulation]) (3.18.0)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.11/dist-packages (from ray==2.31.0->flwr[simulation]) (4.23.0)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ray==2.31.0->flwr[simulation]) (1.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from ray==2.31.0->flwr[simulation]) (24.2)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.11/dist-packages (from ray==2.31.0->flwr[simulation]) (1.3.2)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.11/dist-packages (from ray==2.31.0->flwr[simulation]) (1.5.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography<45.0.0,>=44.0.1->flwr[simulation]) (1.17.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->flwr[simulation]) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->flwr[simulation]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->flwr[simulation]) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->flwr[simulation]) (2025.1.31)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich<14.0.0,>=13.5.0->flwr[simulation]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich<14.0.0,>=13.5.0->flwr[simulation]) (2.18.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from typer<0.13.0,>=0.12.5->flwr[simulation]) (4.13.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<0.13.0,>=0.12.5->flwr[simulation]) (1.5.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography<45.0.0,>=44.0.1->flwr[simulation]) (2.22)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.5.0->flwr[simulation]) (0.1.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray==2.31.0->flwr[simulation]) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray==2.31.0->flwr[simulation]) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray==2.31.0->flwr[simulation]) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray==2.31.0->flwr[simulation]) (0.24.0)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'get_dataset' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-52529e17a6d9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[0;31m# Dataset preparation and training launch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0mNUM_CLIENTS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m \u001b[0mtrainloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_partitions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNUM_CLIENTS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m \u001b[0mclient_fn_callback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_client_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalloaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m strategy = SaveModelStrategy(\n",
            "\u001b[0;32m<ipython-input-2-52529e17a6d9>\u001b[0m in \u001b[0;36mprepare_dataset\u001b[0;34m(num_partitions, batch_size, val_ratio)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m# get the datatset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mtrainset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabnormal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mabnormal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_split_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_split_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'get_dataset' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"Federated Learning Simulation for ECG Classification using CNN\"\"\"\n",
        "\n",
        "# Mount Google Drive to access datasets\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Install required packages\n",
        "!pip install h5py typing-extensions wheel\n",
        "!pip install -U flwr[\"simulation\"]\n",
        "\n",
        "# Import system monitoring and timing utilities\n",
        "import tracemalloc\n",
        "import time\n",
        "\n",
        "# Start memory tracking\n",
        "tracemalloc.start()\n",
        "\n",
        "# Import necessary libraries\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, SubsetRandomSampler, TensorDataset\n",
        "from torch import nn, optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Set hyperparameters\n",
        "batch_size = 500\n",
        "lr = 3e-3\n",
        "epochs = 21\n",
        "val_split_factor = 0.2\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Dataset preparation function\n",
        "def prepare__dataset(abnormal: pd.DataFrame, normal: pd.DataFrame, val_split_factor: float) -> tuple:\n",
        "    \"\"\"\n",
        "    Prepares training and validation datasets by combining abnormal and normal ECG samples.\n",
        "\n",
        "    Args:\n",
        "        abnormal (pd.DataFrame): Abnormal ECG samples.\n",
        "        normal (pd.DataFrame): Normal ECG samples.\n",
        "        val_split_factor (float): Fraction of the dataset to reserve for validation.\n",
        "\n",
        "    Returns:\n",
        "        Tuple containing training set, validation set, and example counts.\n",
        "    \"\"\"\n",
        "    # Drop label column (assumed to be at index 187)\n",
        "    abnormal = abnormal.drop([187], axis=1)\n",
        "    normal = normal.drop([187], axis=1)\n",
        "\n",
        "    # Create target labels (1 for abnormal, 0 for normal)\n",
        "    y_abnormal = pd.DataFrame(np.ones((abnormal.shape[0])))\n",
        "    y_normal = pd.DataFrame(np.zeros((normal.shape[0])))\n",
        "\n",
        "    # Combine data and labels\n",
        "    x = pd.concat([abnormal, normal], sort=True).to_numpy()\n",
        "    y = pd.concat([y_abnormal, y_normal], sort=True)[0].to_numpy()\n",
        "\n",
        "    # Convert to tensor dataset\n",
        "    full_dataset = TensorDataset(torch.from_numpy(x).float(), torch.from_numpy(y).long())\n",
        "\n",
        "    # Shuffle and split dataset\n",
        "    full_len = len(full_dataset)\n",
        "    val_len = int(full_len * val_split_factor)\n",
        "    train_len = full_len - val_len\n",
        "    indices = torch.randperm(full_len).tolist()\n",
        "    train_indices = indices[:train_len]\n",
        "    val_indices = indices[train_len:]\n",
        "\n",
        "    # Create samplers and data loaders\n",
        "    train_sampler = SubsetRandomSampler(train_indices)\n",
        "    val_sampler = SubsetRandomSampler(val_indices)\n",
        "    train_loader = DataLoader(full_dataset, batch_size=train_len, sampler=train_sampler)\n",
        "    val_loader = DataLoader(full_dataset, batch_size=val_len, sampler=val_sampler)\n",
        "\n",
        "    # Extract single batch (entire dataset at once)\n",
        "    train_dataset = list(train_loader)[0]\n",
        "    val_dataset = list(val_loader)[0]\n",
        "\n",
        "    num_examples = {'trainset': train_len, 'testset': val_len}\n",
        "\n",
        "    return train_dataset, val_dataset, num_examples\n",
        "\n",
        "# Load ECG datasets from Google Drive\n",
        "root = '/content/drive/MyDrive/Federated-ECG/ECG_Classification/client/datasets/'\n",
        "abnormal = pd.read_csv(root + 'ptbdb_abnormal.csv', header=None)\n",
        "normal = pd.read_csv(root + 'ptbdb_normal.csv', header=None)\n",
        "\n",
        "# Prepare training and validation datasets\n",
        "train_dataset, val_dataset, _ = prepare__dataset(abnormal=abnormal, normal=normal, val_split_factor=val_split_factor)\n",
        "\n",
        "# Print dataset shapes for verification\n",
        "print(\"Abnormal samples shape:\", abnormal.shape)\n",
        "print(\"Normal samples shape:\", normal.shape)\n",
        "\n",
        "# Define 1D CNN model for ECG classification\n",
        "class ecg_net(nn.Module):\n",
        "    def __init__(self, num_of_class: int):\n",
        "        super(ecg_net, self).__init__()\n",
        "\n",
        "        # Convolutional feature extractor\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv1d(1, 16, kernel_size=3, stride=1, padding=1),\n",
        "            nn.MaxPool1d(2),\n",
        "\n",
        "            nn.Conv1d(16, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.MaxPool1d(2),\n",
        "\n",
        "            nn.Conv1d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.MaxPool1d(2),\n",
        "        )\n",
        "\n",
        "        # Fully connected layers for classification\n",
        "        self.linear = nn.Sequential(\n",
        "            nn.Linear(2944, 500),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "            nn.Linear(500, num_of_class),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)              # Add channel dimension: [B, 1, 187]\n",
        "        x = self.model(x)               # Pass through CNN layers\n",
        "        x = x.view(x.size(0), -1)       # Flatten features\n",
        "        x = self.linear(x)              # Classification head\n",
        "        return x\n",
        "\n",
        "\n",
        "\"\"\"Federated Learning Simulation: One Client, One Data Partition\n",
        "\n",
        "In FL, each client must work with its own local data. This script simulates that setup by:\n",
        "1. Preparing and combining ECG data.\n",
        "2. Splitting it into N partitions (one per client).\n",
        "3. Creating train/val/test loaders per client for simulation purposes.\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def create_combined_dataset_from_raw(abnormal_df, normal_df, val_split_ratio):\n",
        "    \"\"\"\n",
        "    Combines abnormal and normal ECG samples, assigns labels, and returns train/validation datasets.\n",
        "\n",
        "    Args:\n",
        "        abnormal_df (pd.DataFrame): Abnormal ECG data.\n",
        "        normal_df (pd.DataFrame): Normal ECG data.\n",
        "        val_split_ratio (float): Fraction of data reserved for validation.\n",
        "\n",
        "    Returns:\n",
        "        Tuple: train_dataset, val_dataset, and dictionary of data sizes.\n",
        "    \"\"\"\n",
        "    # Remove the label column (assumed at index 187)\n",
        "    abnormal_df = abnormal_df.drop([187], axis=1)\n",
        "    normal_df = normal_df.drop([187], axis=1)\n",
        "\n",
        "    # Label assignment: 1 = abnormal, 0 = normal\n",
        "    y_abnormal = pd.DataFrame(np.ones((abnormal_df.shape[0])))\n",
        "    y_normal = pd.DataFrame(np.zeros((normal_df.shape[0])))\n",
        "\n",
        "    # Concatenate features and labels\n",
        "    x = pd.concat([abnormal_df, normal_df], sort=True).to_numpy()\n",
        "    y = pd.concat([y_abnormal, y_normal], sort=True)[0].to_numpy()\n",
        "\n",
        "    # Wrap into a TensorDataset\n",
        "    full_dataset = TensorDataset(torch.from_numpy(x).float(), torch.from_numpy(y).long())\n",
        "\n",
        "    # Split into training and validation\n",
        "    total_len = len(full_dataset)\n",
        "    val_len = int(total_len * val_split_ratio)\n",
        "    train_len = total_len - val_len\n",
        "\n",
        "    train_dataset, val_dataset = random_split(full_dataset, [train_len, val_len])\n",
        "\n",
        "    return train_dataset, val_dataset, {'trainset': train_len, 'valset': val_len}\n",
        "\n",
        "\n",
        "def load_and_split_data_from_csv(train_path, test_path, batch_size=100, val_split_ratio=0.2):\n",
        "    \"\"\"\n",
        "    Loads preprocessed ECG datasets from CSV files and returns data loaders.\n",
        "\n",
        "    Args:\n",
        "        train_path (str): Path to training CSV.\n",
        "        test_path (str): Path to test CSV.\n",
        "        batch_size (int): Batch size for DataLoaders.\n",
        "        val_split_ratio (float): Fraction of training data to use for validation.\n",
        "\n",
        "    Returns:\n",
        "        Tuple: train_loader, val_loader, test_loader, and data counts.\n",
        "    \"\"\"\n",
        "    train_df = pd.read_csv(train_path, header=None)\n",
        "    test_df = pd.read_csv(test_path, header=None)\n",
        "\n",
        "    # Convert to NumPy arrays\n",
        "    train_data = train_df.to_numpy()\n",
        "    test_data = test_df.to_numpy()\n",
        "\n",
        "    # Create TensorDatasets\n",
        "    train_dataset = TensorDataset(torch.from_numpy(train_data[:, :-1]).float(),\n",
        "                                  torch.from_numpy(train_data[:, -1]).long())\n",
        "    test_dataset = TensorDataset(torch.from_numpy(test_data[:, :-1]).float(),\n",
        "                                 torch.from_numpy(test_data[:, -1]).long())\n",
        "\n",
        "    # Train/validation split\n",
        "    total_len = len(train_dataset)\n",
        "    val_len = int(total_len * val_split_ratio)\n",
        "    train_len = total_len - val_len\n",
        "    train_subset, val_subset = random_split(train_dataset, [train_len, val_len])\n",
        "\n",
        "    # DataLoaders\n",
        "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    return train_loader, val_loader, test_loader, {'trainset': train_len, 'valset': val_len}\n",
        "\n",
        "\n",
        "def simulate_fl_dataset_partitioning(num_clients, batch_size, val_ratio=0.1):\n",
        "    \"\"\"\n",
        "    Simulates FL environment by partitioning the training set into local datasets for each client.\n",
        "\n",
        "    Each client receives a disjoint train+val split, and the test set is shared for evaluation.\n",
        "\n",
        "    Args:\n",
        "        num_clients (int): Number of simulated clients.\n",
        "        batch_size (int): Batch size per client DataLoader.\n",
        "        val_ratio (float): Portion of each client’s local data used for validation.\n",
        "\n",
        "    Returns:\n",
        "        Tuple: List of trainloaders, valloaders for all clients, and a testloader.\n",
        "    \"\"\"\n",
        "\n",
        "    # Use global abnormal/normal data assumed to be already loaded\n",
        "    global abnormal, normal, val_split_factor\n",
        "\n",
        "    # Step 1: Combine data and perform initial train/val split\n",
        "    full_trainset, global_valset, _ = create_combined_dataset_from_raw(\n",
        "        abnormal_df=abnormal,\n",
        "        normal_df=normal,\n",
        "        val_split_ratio=val_split_factor\n",
        "    )\n",
        "\n",
        "    print(\"Total training examples after initial split:\", len(full_trainset))\n",
        "    print(\"Global validation examples:\", len(global_valset))\n",
        "\n",
        "    # Step 2: Divide the training set among clients\n",
        "    partition_size = len(full_trainset) // num_clients\n",
        "    lengths = [partition_size] * num_clients\n",
        "    lengths[-1] += len(full_trainset) - sum(lengths)  # Adjust for rounding\n",
        "\n",
        "    client_trainsets = random_split(full_trainset, lengths, generator=torch.Generator().manual_seed(2023))\n",
        "    print(\"Partition sizes per client:\", [len(p) for p in client_trainsets])\n",
        "\n",
        "    # Step 3: Per-client train/val split + DataLoaders\n",
        "    trainloaders, valloaders = [], []\n",
        "\n",
        "    for client_data in client_trainsets:\n",
        "        total_len = len(client_data)\n",
        "        val_len = int(total_len * val_ratio)\n",
        "        train_len = total_len - val_len\n",
        "\n",
        "        local_train, local_val = random_split(client_data, [train_len, val_len],\n",
        "                                              generator=torch.Generator().manual_seed(2023))\n",
        "\n",
        "        trainloaders.append(DataLoader(local_train, batch_size=batch_size, shuffle=True))\n",
        "        valloaders.append(DataLoader(local_val, batch_size=batch_size, shuffle=False))\n",
        "\n",
        "    # Step 4: Create a test loader from the global validation set\n",
        "    testloader = DataLoader(global_valset, batch_size=128)\n",
        "\n",
        "    # Log example datapoint size\n",
        "    print('Sample input vector length:', len(full_trainset[0][0]))\n",
        "\n",
        "    return trainloaders, valloaders, testloader\n",
        "\n",
        "\n",
        "# ========== Example Usage ========== #\n",
        "\n",
        "# Define global variables\n",
        "val_split_factor = 0.2\n",
        "NUM_CLIENTS = 2\n",
        "\n",
        "# Load dataset (replace with actual CSV paths if needed)\n",
        "root = '/content/drive/MyDrive/Federated-ECG/ECG_Classification/client/datasets/'\n",
        "abnormal = pd.read_csv(root + 'ptbdb_abnormal.csv', header=None)\n",
        "normal = pd.read_csv(root + 'ptbdb_normal.csv', header=None)\n",
        "\n",
        "# Prepare the client loaders\n",
        "trainloaders, valloaders, testloader = simulate_fl_dataset_partitioning(\n",
        "    num_clients=NUM_CLIENTS,\n",
        "    batch_size=20,\n",
        "    val_ratio=0.1\n",
        ")\n",
        "\n",
        "\n",
        "# !pip install -U flwr[\"simulation\"]\n",
        "\n",
        "import flwr as fl\n",
        "\n",
        "# !cp /content/drive/MyDrive/TNR\\ Lab/Federated-ECG/simulate_fl/client_train.py /content\n",
        "\n",
        "\"\"\"Model\"\"\"\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import nn,optim\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#define the ecg_net model\n",
        "class ecg_net(nn.Module):\n",
        "\n",
        "    def __init__(self, num_of_class):\n",
        "        super(ecg_net, self).__init__()\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv1d(1, 16, kernel_size=3, stride=1, padding=1),\n",
        "            nn.MaxPool1d(2),\n",
        "\n",
        "            nn.Conv1d(16, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.MaxPool1d(2),\n",
        "\n",
        "            nn.Conv1d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.MaxPool1d(2),\n",
        "\n",
        "        )\n",
        "\n",
        "        self.linear = nn.Sequential(\n",
        "            nn.Linear(2944,500),\n",
        "            nn.LeakyReLU(inplace=True),\n",
        "            nn.Linear(500, num_of_class),\n",
        "\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = x.unsqueeze(1)\n",
        "        x = self.model(x)\n",
        "        # print(x.shape)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        #x [b, 2944]\n",
        "        # print(x.shape)\n",
        "        x = self.linear(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# hyperparameters\n",
        "batch_size=1\n",
        "lr = 3e-3\n",
        "epochs = 10\n",
        "val_split_factor = 0.2\n",
        "torch.manual_seed(1234)\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"using {} device.\".format(device))\n",
        "\n",
        "\"\"\"Client\"\"\"\n",
        "\n",
        "def evalute(model, loader):\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = len(loader)\n",
        "    val_bar = tqdm(loader, file=sys.stdout)\n",
        "    for x, y in val_bar:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        with torch.no_grad():\n",
        "            logits = model(x)\n",
        "            pred = logits.argmax(dim=1)\n",
        "        correct += torch.eq(pred, y).sum().float().item()\n",
        "\n",
        "    return correct / total\n",
        "\n",
        "def train_client(model, train_loader, valid_loader, epochs=1):\n",
        "\n",
        "    # model = ecg_net(2).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    criteon = nn.CrossEntropyLoss()\n",
        "\n",
        "    best_acc, best_epoch = 0, 0\n",
        "    global_step = 0\n",
        "\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        train_bar = tqdm(train_loader, file=sys.stdout)\n",
        "        for step, (x, y) in enumerate(train_bar):\n",
        "            # x: [b, 187], y: [b]\n",
        "            x, y = x.to(device), y.to(device)\n",
        "\n",
        "            model.train()\n",
        "\n",
        "            logits = model(x)\n",
        "            loss = criteon(logits, y)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "\n",
        "            # for param in model.parameters():\n",
        "            #     print(param.grad)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            train_bar.desc = \"train epoch[{}/{}] loss:{:.3f}\".format(epoch + 1,\n",
        "                                                                     epochs,\n",
        "                                                                     loss)\n",
        "\n",
        "            global_step += 1\n",
        "\n",
        "        if epoch % 1 == 0:  # You can change the validation frequency as you wish\n",
        "\n",
        "            val_acc = evalute(model, valid_loader)\n",
        "\n",
        "            print('val_acc = ',val_acc)\n",
        "            if val_acc > best_acc:\n",
        "                best_epoch = epoch\n",
        "                best_acc = val_acc\n",
        "\n",
        "                # torch.save(model.state_dict(), 'best_client_model.mdl')\n",
        "\n",
        "        print(\"Global steps\", global_step)\n",
        "\n",
        "    print('best acc:', best_acc, 'best epoch:', best_epoch)\n",
        "\n",
        "from collections import OrderedDict\n",
        "from typing import Dict, List, Tuple\n",
        "import torch\n",
        "from flwr.common import NDArrays, Scalar\n",
        "\n",
        "class FlowerClient(fl.client.NumPyClient):\n",
        "    def __init__(self, trainloader, vallodaer) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.trainloader = trainloader\n",
        "        self.valloader = vallodaer\n",
        "        self.model = ecg_net(2)\n",
        "        # Determine device\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model.to(self.device)  # send model to device\n",
        "\n",
        "    def set_parameters(self, parameters):\n",
        "        \"\"\"With the model paramters received from the server,\n",
        "        overwrite the uninitialise model in this class with them.\"\"\"\n",
        "\n",
        "        params_dict = zip(self.model.state_dict().keys(), parameters)\n",
        "        state_dict = OrderedDict({k: torch.Tensor(v) for k, v in params_dict})\n",
        "        # now replace the parameters\n",
        "        self.model.load_state_dict(state_dict, strict=True)\n",
        "\n",
        "    def get_parameters(self, config: Dict[str, Scalar]):\n",
        "        \"\"\"Extract all model parameters and conver them to a list of\n",
        "        NumPy arryas. The server doesn't work with PyTorch/TF/etc.\"\"\"\n",
        "        return [val.cpu().numpy() for _, val in self.model.state_dict().items()]\n",
        "\n",
        "    def fit(self, parameters, config):\n",
        "        \"\"\"This method train the model using the parameters sent by the\n",
        "        server on the dataset of this client. At then end, the parameters\n",
        "        of the locally trained model are communicated back to the server\"\"\"\n",
        "\n",
        "        # copy parameters sent by the server into client's local model\n",
        "        self.set_parameters(parameters)\n",
        "\n",
        "        # read from config\n",
        "        lr = 0.001 # config[\"lr\"]\n",
        "        epochs = 5 #config[\"epochs\"]\n",
        "\n",
        "        # Define the optimizer\n",
        "        optim = torch.optim.SGD(self.model.parameters(), lr=lr, momentum=0.9)\n",
        "\n",
        "        # do local training\n",
        "        train_client(self.model, self.trainloader, self.valloader, epochs=epochs)\n",
        "\n",
        "        # return the model parameters to the server as well as extra info (number of training examples in this case)\n",
        "        return self.get_parameters({}), len(self.trainloader), {}\n",
        "\n",
        "    def evaluate(self, parameters: NDArrays, config: Dict[str, Scalar]) -> Tuple[float, int, Dict[str, Scalar]]:\n",
        "        \"\"\"Evaluate the model on the locally held dataset.\"\"\"\n",
        "\n",
        "        # Update local model with parameters received from the server\n",
        "        self.set_parameters(parameters)\n",
        "\n",
        "        # Get the loss criterion\n",
        "        criterion = torch.nn.CrossEntropyLoss()\n",
        "        total_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        # Switch the model to evaluation mode\n",
        "        self.model.eval()\n",
        "\n",
        "        # Disable gradient calculation during evaluation\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in self.valloader:\n",
        "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
        "                outputs = self.model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                total_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        # Calculate average loss and accuracy\n",
        "        avg_loss = total_loss / len(self.valloader)\n",
        "        accuracy = correct / total\n",
        "\n",
        "        # Return evaluation results\n",
        "        return avg_loss, total, {\"accuracy\": accuracy}\n",
        "\n",
        "model = ecg_net(2).to(device=device)\n",
        "client = FlowerClient(trainloaders[0], valloaders[0])\n",
        "\n",
        "\"\"\"Server\"\"\"\n",
        "\n",
        "from typing import Dict, Optional, Tuple, List, Union\n",
        "from collections import OrderedDict\n",
        "import flwr as fl\n",
        "from flwr.common import Scalar, Metrics\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import time\n",
        "import tracemalloc\n",
        "\n",
        "\n",
        "# ------------------ Evaluation Function ------------------\n",
        "\n",
        "def evaluate(\n",
        "    server_round: int,\n",
        "    parameters: fl.common.NDArrays,\n",
        "    config: Dict[str, Scalar],\n",
        ") -> Optional[Tuple[float, Dict[str, Scalar]]]:\n",
        "    \"\"\"\n",
        "    Custom evaluation function executed on the server after each round.\n",
        "    Aggregates validation results from all clients.\n",
        "    \"\"\"\n",
        "    # Load global parameters into model\n",
        "    params_dict = zip(model.state_dict().keys(), parameters)\n",
        "    state_dict = OrderedDict({k: torch.tensor(v) for k, v in params_dict})\n",
        "    model.load_state_dict(state_dict, strict=True)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    total_loss, total_correct, total_samples = 0.0, 0, 0\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for valloader in valloaders:\n",
        "            for inputs, labels in valloader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                total_loss += loss.item() * inputs.size(0)\n",
        "                total_correct += (outputs.argmax(1) == labels).sum().item()\n",
        "                total_samples += labels.size(0)\n",
        "\n",
        "    avg_loss = total_loss / total_samples\n",
        "    accuracy = total_correct / total_samples\n",
        "\n",
        "    return avg_loss, {\"accuracy\": accuracy}\n",
        "\n",
        "\n",
        "# ------------------ Per-Round Config ------------------\n",
        "\n",
        "def fit_config(server_round: int) -> Dict[str, Scalar]:\n",
        "    \"\"\"\n",
        "    Defines training config (e.g. local epochs, batch size) for each client per round.\n",
        "    \"\"\"\n",
        "    return {\n",
        "        \"batch_size\": 1,\n",
        "        \"local_epochs\": 1 if server_round < 2 else 2,\n",
        "    }\n",
        "\n",
        "\n",
        "# ------------------ Metrics Aggregation ------------------\n",
        "\n",
        "def weighted_average(metrics: List[Tuple[int, Metrics]]) -> Metrics:\n",
        "    \"\"\"\n",
        "    Aggregates evaluation metrics (e.g. accuracy) across clients using weighted average.\n",
        "    \"\"\"\n",
        "    accuracies = [num_examples * m[\"accuracy\"] for num_examples, m in metrics]\n",
        "    total_examples = sum(num_examples for num_examples, _ in metrics)\n",
        "    return {\"accuracy\": sum(accuracies) / total_examples}\n",
        "\n",
        "\n",
        "# ------------------ Custom Strategy with Model Saving ------------------\n",
        "\n",
        "class SaveModelStrategy(fl.server.strategy.FedAvg):\n",
        "    def aggregate_fit(\n",
        "        self,\n",
        "        server_round: int,\n",
        "        results: List[Tuple[fl.server.client_proxy.ClientProxy, fl.common.FitRes]],\n",
        "        failures: List[Union[Tuple[fl.server.client_proxy.ClientProxy, fl.common.FitRes], BaseException]],\n",
        "    ) -> Tuple[Optional[fl.common.Parameters], Dict[str, Scalar]]:\n",
        "\n",
        "        # Call base FedAvg to get aggregated model parameters\n",
        "        aggregated_parameters, aggregated_metrics = super().aggregate_fit(server_round, results, failures)\n",
        "\n",
        "        if aggregated_parameters is not None:\n",
        "            print(f\"Saving model for round {server_round}...\")\n",
        "\n",
        "            # Convert parameters to NumPy arrays and save\n",
        "            aggregated_ndarrays: List[np.ndarray] = fl.common.parameters_to_ndarrays(aggregated_parameters)\n",
        "            np.savez(f\"round-{server_round}-weights.npz\", *aggregated_ndarrays)\n",
        "\n",
        "            # Load parameters into model and save state dict\n",
        "            state_dict = OrderedDict({\n",
        "                k: torch.tensor(v) for k, v in zip(model.state_dict().keys(), aggregated_ndarrays)\n",
        "            })\n",
        "            model.load_state_dict(state_dict, strict=True)\n",
        "            torch.save(model.state_dict(), f\"model_round_{server_round}.pth\")\n",
        "\n",
        "        return aggregated_parameters, aggregated_metrics\n",
        "\n",
        "\n",
        "# ------------------ Client Generator ------------------\n",
        "\n",
        "def generate_client_fn(trainloaders, valloaders):\n",
        "    \"\"\"\n",
        "    Returns a client creation function to be used by Flower's virtual client engine.\n",
        "    \"\"\"\n",
        "    def client_fn(cid: str):\n",
        "        return FlowerClient(\n",
        "            trainloader=trainloaders[int(cid)],\n",
        "            vallodaer=valloaders[int(cid)]\n",
        "        )\n",
        "    return client_fn\n",
        "\n",
        "\n",
        "# ------------------ Launch Simulation ------------------\n",
        "\n",
        "# Instantiate the strategy with custom configuration\n",
        "strategy = SaveModelStrategy(\n",
        "    fraction_fit=1.0,\n",
        "    min_fit_clients=2,\n",
        "    min_available_clients=2,\n",
        "    evaluate_fn=evaluate,\n",
        "    on_fit_config_fn=fit_config,\n",
        "    evaluate_metrics_aggregation_fn=weighted_average,\n",
        ")\n",
        "\n",
        "# Generate clients and simulate training\n",
        "client_fn_callback = generate_client_fn(trainloaders, valloaders)\n",
        "client_resources = {\"num_cpus\": 1, \"num_gpus\": 1}\n",
        "\n",
        "# Start memory tracking and training\n",
        "tracemalloc.start()\n",
        "start_training = time.time()\n",
        "\n",
        "fl.simulation.start_simulation(\n",
        "    client_fn=client_fn_callback,\n",
        "    num_clients=NUM_CLIENTS,\n",
        "    config=fl.server.ServerConfig(num_rounds=3),\n",
        "    strategy=strategy,\n",
        "    client_resources=client_resources,\n",
        "    ray_init_args={\"log_to_driver\": False, \"num_cpus\": 1, \"num_gpus\": 1},\n",
        ")\n",
        "\n",
        "end_training = time.time()\n",
        "\n",
        "# ------------------ Training Summary ------------------\n",
        "\n",
        "print(f\"Total Training Time: {end_training - start_training:.2f} seconds\")\n",
        "\n",
        "current, peak = tracemalloc.get_traced_memory()\n",
        "print(f\"Current memory usage: {current / (1024**2):.2f} MB\")\n",
        "print(f\"Peak memory usage: {peak / (1024**2):.2f} MB\")\n",
        "\n",
        "tracemalloc.stop()"
      ],
      "metadata": {
        "id": "NPGkg9lflVW2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "144368df-978a-46eb-df25-90d49728417d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (3.13.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (4.13.1)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (0.45.1)\n",
            "Requirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.11/dist-packages (from h5py) (2.0.2)\n",
            "Requirement already satisfied: flwr[simulation] in /usr/local/lib/python3.11/dist-packages (1.17.0)\n",
            "Requirement already satisfied: cryptography<45.0.0,>=44.0.1 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (44.0.2)\n",
            "Requirement already satisfied: grpcio!=1.65.0,<2.0.0,>=1.62.3 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (1.71.0)\n",
            "Requirement already satisfied: iterators<0.0.3,>=0.0.2 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (0.0.2)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (2.0.2)\n",
            "Requirement already satisfied: pathspec<0.13.0,>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (0.12.1)\n",
            "Requirement already satisfied: protobuf<5.0.0,>=4.21.6 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (4.25.6)\n",
            "Requirement already satisfied: pycryptodome<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (3.22.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=6.0.2 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (6.0.2)\n",
            "Requirement already satisfied: ray==2.31.0 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (2.31.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (2.32.3)\n",
            "Requirement already satisfied: rich<14.0.0,>=13.5.0 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (13.9.4)\n",
            "Requirement already satisfied: tomli<3.0.0,>=2.0.1 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (2.2.1)\n",
            "Requirement already satisfied: tomli-w<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (1.2.0)\n",
            "Requirement already satisfied: typer<0.13.0,>=0.12.5 in /usr/local/lib/python3.11/dist-packages (from flwr[simulation]) (0.12.5)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from ray==2.31.0->flwr[simulation]) (8.1.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from ray==2.31.0->flwr[simulation]) (3.18.0)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.11/dist-packages (from ray==2.31.0->flwr[simulation]) (4.23.0)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ray==2.31.0->flwr[simulation]) (1.1.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from ray==2.31.0->flwr[simulation]) (24.2)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.11/dist-packages (from ray==2.31.0->flwr[simulation]) (1.3.2)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.11/dist-packages (from ray==2.31.0->flwr[simulation]) (1.5.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography<45.0.0,>=44.0.1->flwr[simulation]) (1.17.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->flwr[simulation]) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->flwr[simulation]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->flwr[simulation]) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.31.0->flwr[simulation]) (2025.1.31)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich<14.0.0,>=13.5.0->flwr[simulation]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich<14.0.0,>=13.5.0->flwr[simulation]) (2.18.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from typer<0.13.0,>=0.12.5->flwr[simulation]) (4.13.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<0.13.0,>=0.12.5->flwr[simulation]) (1.5.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography<45.0.0,>=44.0.1->flwr[simulation]) (2.22)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.5.0->flwr[simulation]) (0.1.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray==2.31.0->flwr[simulation]) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray==2.31.0->flwr[simulation]) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray==2.31.0->flwr[simulation]) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray==2.31.0->flwr[simulation]) (0.24.0)\n",
            "Abnormal samples shape: (10518, 188)\n",
            "Normal samples shape: (4052, 188)\n",
            "Total training examples after initial split: 11656\n",
            "Global validation examples: 2914\n",
            "Partition sizes per client: [5828, 5828]\n",
            "Sample input vector length: 187\n",
            "using cpu device.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: flwr.simulation.start_simulation() is deprecated.\n",
            "\tInstead, use the `flwr run` CLI command to start a local simulation in your Flower app, as shown for example below:\n",
            "\n",
            "\t\t$ flwr new  # Create a new Flower app from a template\n",
            "\n",
            "\t\t$ flwr run  # Run the Flower app in Simulation Mode\n",
            "\n",
            "\tUsing `start_simulation()` is deprecated.\n",
            "\n",
            "            This is a deprecated feature. It will be removed\n",
            "            entirely in future versions of Flower.\n",
            "        \n",
            "\u001b[92mINFO \u001b[0m:      Starting Flower simulation, config: num_rounds=3, no round_timeout\n",
            "2025-04-12 19:18:38,229\tINFO worker.py:1771 -- Started a local Ray instance.\n",
            "\u001b[92mINFO \u001b[0m:      Flower VCE: Ray initialized with resources: {'CPU': 1.0, 'object_store_memory': 3993989529.0, 'node:172.28.0.12': 1.0, 'memory': 7987979060.0, 'GPU': 1.0, 'node:__internal_head__': 1.0}\n",
            "\u001b[92mINFO \u001b[0m:      Optimize your simulation with Flower VCE: https://flower.ai/docs/framework/how-to-run-simulations.html\n",
            "\u001b[92mINFO \u001b[0m:      Flower VCE: Resources for each Virtual Client: {'num_cpus': 1, 'num_gpus': 1}\n",
            "\u001b[92mINFO \u001b[0m:      Flower VCE: Creating VirtualClientEngineActorPool with 1 actors\n",
            "\u001b[92mINFO \u001b[0m:      [INIT]\n",
            "\u001b[92mINFO \u001b[0m:      Requesting initial parameters from one random client\n",
            "\u001b[92mINFO \u001b[0m:      Received initial parameters from one random client\n",
            "\u001b[92mINFO \u001b[0m:      Starting evaluation of initial global parameters\n",
            "\u001b[92mINFO \u001b[0m:      initial parameters (loss, other metrics): 0.7211126998527763, {'accuracy': 0.281786941580756}\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 1]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 2 clients (out of 2)\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 2 results and 0 failures\n",
            "\u001b[93mWARNING \u001b[0m:   No fit_metrics_aggregation_fn provided\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving model for round 1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[92mINFO \u001b[0m:      fit progress: (1, 0.49853538033069206, {'accuracy': 0.7946735395189003}, 96.29185228700044)\n",
            "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 2 clients (out of 2)\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 2 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 2]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 2 clients (out of 2)\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 2 results and 0 failures\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving model for round 2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[92mINFO \u001b[0m:      fit progress: (2, 0.4080029101496824, {'accuracy': 0.8316151202749141}, 189.9826537680001)\n",
            "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 2 clients (out of 2)\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 2 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 3]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 2 clients (out of 2)\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 2 results and 0 failures\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving model for round 3...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[92mINFO \u001b[0m:      fit progress: (3, 0.19175772078469597, {'accuracy': 0.9329896907216495}, 285.3318970850005)\n",
            "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 2 clients (out of 2)\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 2 results and 0 failures\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [SUMMARY]\n",
            "\u001b[92mINFO \u001b[0m:      Run finished 3 round(s) in 286.53s\n",
            "\u001b[92mINFO \u001b[0m:      \tHistory (loss, distributed):\n",
            "\u001b[92mINFO \u001b[0m:      \t\tround 1: 0.49724871168533963\n",
            "\u001b[92mINFO \u001b[0m:      \t\tround 2: 0.39643089690418193\n",
            "\u001b[92mINFO \u001b[0m:      \t\tround 3: 0.18616001289336828\n",
            "\u001b[92mINFO \u001b[0m:      \tHistory (loss, centralized):\n",
            "\u001b[92mINFO \u001b[0m:      \t\tround 0: 0.7211126998527763\n",
            "\u001b[92mINFO \u001b[0m:      \t\tround 1: 0.49853538033069206\n",
            "\u001b[92mINFO \u001b[0m:      \t\tround 2: 0.4080029101496824\n",
            "\u001b[92mINFO \u001b[0m:      \t\tround 3: 0.19175772078469597\n",
            "\u001b[92mINFO \u001b[0m:      \tHistory (metrics, distributed, evaluate):\n",
            "\u001b[92mINFO \u001b[0m:      \t{'accuracy': [(1, 0.7946735395189003),\n",
            "\u001b[92mINFO \u001b[0m:      \t              (2, 0.8316151202749141),\n",
            "\u001b[92mINFO \u001b[0m:      \t              (3, 0.9329896907216495)]}\n",
            "\u001b[92mINFO \u001b[0m:      \tHistory (metrics, centralized):\n",
            "\u001b[92mINFO \u001b[0m:      \t{'accuracy': [(0, 0.281786941580756),\n",
            "\u001b[92mINFO \u001b[0m:      \t              (1, 0.7946735395189003),\n",
            "\u001b[92mINFO \u001b[0m:      \t              (2, 0.8316151202749141),\n",
            "\u001b[92mINFO \u001b[0m:      \t              (3, 0.9329896907216495)]}\n",
            "\u001b[92mINFO \u001b[0m:      \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Training Time: 309.05 seconds\n",
            "Current memory usage: 23.21 MB\n",
            "Peak memory usage: 89.56 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wDRUweatGHL9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "pytorch",
      "language": "python",
      "name": "pytorch"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}